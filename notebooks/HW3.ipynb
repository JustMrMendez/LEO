{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2e4f0e",
   "metadata": {},
   "source": [
    "# CAP5610 HW3 — Tree Ensembles & SHAP Study\n",
    "\n",
    "This notebook mirrors the Homework 3 brief and documents the full experimental path: data curation, model selection, and SHAP-based interpretation for both the classification and regression tasks. The narrative is intentionally research-style—each section introduces objectives, explains decisions, and records findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1773f2e",
   "metadata": {},
   "source": [
    "## Study Roadmap\n",
    "1. **Environment checks** — load shared helpers, set global knobs, confirm data availability.\n",
    "2. **Task 1: Classification** — inspect the gene-expression matrix, train the tree-based suite, analyse accuracy/F1.\n",
    "3. **Task 2: Classifier SHAP** — extract cancer-specific feature importances and patient-level force plots.\n",
    "4. **Task 3: Regression** — profile the drug-response table, compare regressors using MAE/MSE/RMSE/R².\n",
    "5. **Task 4: Regressor SHAP** — quantify drug biomarkers and zoom in on the best-predicted drug–cell-line pair.\n",
    "6. **Conclusion** — catalogue artefacts and possible extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be923b51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- 0. Environment priming -------------------------------------------------------\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import src.leo.HW3_Mendez as hw3\n",
    "reload(hw3)  # borrow the script implementation but allow iterative tweaks in notebook\n",
    "\n",
    "# Reproducibility knobs (kept conservative to avoid memory spikes on laptops).\n",
    "hw3.MAX_FEATURES_CLASSIF = 1000  # cap genes for Task 1/2\n",
    "hw3.MAX_FEATURES_REGRESS = 1200  # cap features for Task 3/4\n",
    "hw3.SHAP_SAMPLES_PER_CLASS = 30  # per-class sample size for SHAP aggregation\n",
    "hw3.SHAP_SAMPLES_REG = 100       # regression SHAP sampling budget\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "CANCER_PATH = hw3.resolve_data_path(hw3.CANCER_CSV, hw3.CANCER_FALLBACK)\n",
    "REG_PATH = hw3.resolve_data_path(hw3.GDSC2_CSV, hw3.GDSC2_FALLBACK)\n",
    "\n",
    "assert CANCER_PATH is not None, \"Classification CSV missing – checked primary and fallback paths.\"\n",
    "assert REG_PATH is not None, \"Regression CSV missing – checked primary and fallback paths.\"\n",
    "\n",
    "hw3.log(f\"Notebook ready. Using {Path(CANCER_PATH).name} and {Path(REG_PATH).name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0ed1c",
   "metadata": {},
   "source": [
    "## Task 1 — Classification Dataset Reconnaissance\n",
    "We first sanity-check the lncRNA expression matrix: confirm sample size, feature count after variance selection, and the presence of a TCGA identifier column. This mirrors the context section of a research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24682bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification data using the memory-aware helper.\n",
    "Xc, yc, sample_ids, class_col, id_col = hw3.memory_savvy_read_cancers(str(CANCER_PATH), hw3.MAX_FEATURES_CLASSIF)\n",
    "\n",
    "summary_cls = pd.DataFrame(\n",
    "    {\n",
    "        \"rows\": [len(Xc)],\n",
    "        \"selected_features\": [Xc.shape[1]],\n",
    "        \"target_column\": [class_col],\n",
    "        \"id_column\": [id_col],\n",
    "    }\n",
    ")\n",
    "\n",
    "hw3.log(\"Task 1 dataset loaded.\")\n",
    "display(summary_cls)\n",
    "\n",
    "# Inspect a small slice (5 samples × 10 genes) to verify numeric coercion worked as expected.\n",
    "Xc.iloc[:5, :10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061dee47",
   "metadata": {},
   "source": [
    "## Task 1 — Model Comparison Strategy\n",
    "We compare six tree-based classifiers under a shared preprocessing pipeline (median imputation). Stratified splits preserve cancer balance. Metrics emphasise macro-F1 to account for class parity, consistent with the assignment brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd350577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tree ensemble family and collect results.\n",
    "cls_results, cls_models, idx_to_class = hw3.train_compare_classifiers(Xc, yc, hw3.RANDOM_STATE)\n",
    "\n",
    "hw3.log(\"Task 1 model sweep complete.\")\n",
    "display(cls_results)\n",
    "\n",
    "best_cls_name = cls_results.iloc[0][\"Model\"]\n",
    "best_classifier = cls_models[best_cls_name]\n",
    "\n",
    "hw3.log(f\"Task 1 best classifier: {best_cls_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935dfb6",
   "metadata": {},
   "source": [
    "## Task 1 — Confusion Matrix & Per-Class Metrics\n",
    "Documenting per-class precision/recall mirrors the reporting expectations. We reuse the CSV exported by the helper for reproducibility, but also display the confusion matrix inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = pd.read_csv(hw3.OUT_DIR / \"task1_confusion_matrix.csv\", index_col=0)\n",
    "classification_report = pd.read_csv(hw3.OUT_DIR / \"task1_classification_report.csv\", index_col=0)\n",
    "\n",
    "hw3.log(\"Task 1 evaluation artefacts loaded from hw3_outputs/\")\n",
    "display(confusion)\n",
    "classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feef00a",
   "metadata": {},
   "source": [
    "## Task 2 — SHAP Rationale\n",
    "With the winning classifier in hand, we quantify contribution patterns. Mean |SHAP| scores highlight cancer-specific genes, while force plots satisfy the assignment’s patient-level interpretability requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76649b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SHAP analysis with memory-aware sampling.\n",
    "hw3.shap_task2(best_classifier, Xc, yc, sample_ids, hw3.PATIENT_ID_TO_PLOT, idx_to_class)\n",
    "\n",
    "# Display the aggregated top-10 importance list (first 15 rows for brevity).\n",
    "task2_top = pd.read_csv(hw3.OUT_DIR / \"task2a_top10_features_per_cancer.csv\")\n",
    "\n",
    "hw3.log(\"Task 2 SHAP outputs generated.\")\n",
    "task2_top.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c8027",
   "metadata": {},
   "source": [
    "Force plots were saved in `hw3_outputs/task2b_forceplot_*`. Open them in a browser to capture screenshots or embed HTML in the written report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120cdc7c",
   "metadata": {},
   "source": [
    "## Task 3 — Regression Dataset Reconnaissance\n",
    "We repeat the dataset audit for the drug-response table: number of rows, features retained after variance filtering, and ID columns used to define unique drug–cell-line pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30816d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr, yr, keys, meta = hw3.memory_savvy_read_gdsc2(str(REG_PATH), hw3.MAX_FEATURES_REGRESS)\n",
    "\n",
    "summary_reg = pd.DataFrame(\n",
    "    {\n",
    "        \"rows\": [meta[\"n_rows\"]],\n",
    "        \"selected_features\": [meta[\"n_features\"]],\n",
    "        \"target_column\": [meta[\"target\"]],\n",
    "        \"id_columns\": [\" & \".join(meta[\"id_cols\"])],\n",
    "    }\n",
    ")\n",
    "\n",
    "hw3.log(\"Task 3 dataset loaded.\")\n",
    "display(summary_reg)\n",
    "Xr.iloc[:5, :10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8acb51",
   "metadata": {},
   "source": [
    "## Task 3 — Regressor Comparison\n",
    "Analogous to Task 1, we bench the regressors with shared preprocessing. The assignment requests MAE, MSE, RMSE, and R²; we present them in a sortable DataFrame for inclusion in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_results, reg_models = hw3.train_compare_regressors(Xr, yr, hw3.RANDOM_STATE)\n",
    "\n",
    "hw3.log(\"Task 3 model sweep complete.\")\n",
    "display(reg_results)\n",
    "\n",
    "best_reg_name = reg_results.iloc[0][\"Model\"]\n",
    "best_regressor = reg_models[best_reg_name]\n",
    "\n",
    "hw3.log(f\"Task 3 best regressor: {best_reg_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08512c6",
   "metadata": {},
   "source": [
    "## Task 4 — SHAP on Best Regressor\n",
    "We apply TreeExplainer to the winning regressor. Mean |SHAP| per drug corresponds to Task 4a, while Task 4b singles out the drug–cell-line pair with the smallest prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw3.shap_task4(best_regressor, Xr, yr, keys)\n",
    "\n",
    "hw3.log(\"Task 4 SHAP outputs generated.\")\n",
    "\n",
    "per_drug = pd.read_csv(hw3.OUT_DIR / \"task4a_top10_features_per_drug.csv\")\n",
    "least_error_path = sorted(hw3.OUT_DIR.glob(\"task4b_top10_features_least_error_*.csv\"))[-1]\n",
    "least_error = pd.read_csv(least_error_path)\n",
    "\n",
    "(per_drug.head(20), least_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2496be",
   "metadata": {},
   "source": [
    "## Conclusion & Deliverables\n",
    "- Metrics and SHAP artefacts live in `hw3_outputs/` for direct ingestion into the written report.\n",
    "- Adjust the feature caps or SHAP sample budgets at the top cell if you replicate on a workstation with more memory.\n",
    "- Next steps (optional): hyper-parameter tuning around the winning models, or integrating biological annotations for the highlighted genes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}