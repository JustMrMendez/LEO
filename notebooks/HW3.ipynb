{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f791805",
   "metadata": {},
   "source": [
    "# CAP5610 HW3 — Tree Ensembles & SHAP Study\n",
    "\n",
    "This notebook mirrors Homework 3 and is self-contained: with the two datasets (`lncRNA_5_Cancers.csv` and `hw3-drug-screening-data.csv`) placed beside it, you can rerun every block to regenerate the figures, tables, and SHAP artefacts required in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08310979",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet numpy pandas scikit-learn xgboost lightgbm catboost shap psutil 'tqdm[joblib]' polars pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8bed0",
   "metadata": {},
   "source": [
    "## Study Roadmap\n",
    "1. **Shared setup** — import libraries, configure reproducible knobs, and define reusable helpers.\n",
    "2. **Task 1** — explore the classification dataset and benchmark tree-based classifiers.\n",
    "3. **Task 2** — interpret the best classifier with SHAP (per-cancer importances + patient force plots).\n",
    "4. **Task 3** — profile the regression dataset and compare regressors on MAE/MSE/RMSE/R².\n",
    "5. **Task 4** — run SHAP on the winning regressor for drug-specific insights and least-error explanation.\n",
    "6. **Conclusion** — summarise artefacts and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04802d51",
   "metadata": {},
   "source": [
    "## Optimization Progress Tracker\n",
    "| Task | Description | Status |\n",
    "| --- | --- | --- |\n",
    "| Baseline Benchmarking | Measure end-to-end runtime, memory peaks, and per-stage costs for Tasks 1–4. | ☑ Completed |\n",
    "| Data Ingestion Optimizations | Evaluate faster file readers, column pruning, and memory mapping strategies. | ☑ Completed |\n",
    "| Feature Engineering Efficiency | Explore incremental variance selection, sparse representations, and caching pipelines. | ☑ Completed |\n",
    "| Model Training Parallelism | Investigate multi-core settings, histogram optimizations, and distributed runners for each algorithm. | ☑ Completed |\n",
    "| SHAP Acceleration | Profile TreeExplainer usage, test approximate SHAP (e.g., FastTreeSHAP), and batch visualisation. | ☑ Completed |\n",
    "| Experiment Automation | Persistent progress bar + checkpoint system for resumable notebook runs. | ☑ Completed |\n",
    "| Literature & Benchmark Survey | Compile findings from large-scale ML challenges (e.g., 1BR challenge) for applicable techniques. | ☐ Not started |\n",
    "| Implementation Plan | Prioritise quick wins vs. deep refactors; outline test matrix for regression coverage. | ☐ Not started |\n",
    "| Reporting & Validation | Document runtime improvements, ensure parity with assignment outputs, and update report narrative. | ☐ Not started |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f89dc",
   "metadata": {},
   "source": [
    "## 0. Shared Setup & Reusable Utilities\n",
    "The following cell loads all required libraries, defines runtime configuration (random seed, feature caps, SHAP sampling budgets), and introduces helper routines for logging and path resolution. Keeping the helpers here ensures the notebook is standalone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1fe7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment automation:  17%|█▋        | 1/6 [00:00<00:00, 42366.71it/s, Idle]"
     ]
    }
   ],
   "source": [
    "# --- Imports & global configuration -------------------------------------------------\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from functools import wraps\n",
    "from time import perf_counter\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from IPython.display import HTML, display\n",
    "from tqdm.auto import tqdm\n",
    "try:\n",
    "    from tqdm.joblib import tqdm_joblib\n",
    "except Exception:\n",
    "    from contextlib import contextmanager\n",
    "    @contextmanager\n",
    "    def tqdm_joblib(*args, **kwargs):\n",
    "        yield\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    HAVE_XGB = True\n",
    "except Exception:\n",
    "    HAVE_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    HAVE_LGBM = True\n",
    "except Exception:\n",
    "    HAVE_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    HAVE_CAT = True\n",
    "except Exception:\n",
    "    HAVE_CAT = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAVE_SHAP = True\n",
    "except Exception:\n",
    "    HAVE_SHAP = False\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "    HAVE_POLARS = True\n",
    "except Exception:\n",
    "    HAVE_POLARS = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Global experiment knobs — identical to the accompanying report.\n",
    "RANDOM_STATE = 42\n",
    "CANCER_SET = {\"KIRC\", \"LUAD\", \"LUSC\", \"PRAD\", \"THCA\"}\n",
    "PATIENT_ID_TO_PLOT = \"TCGA-39-5011-01A\"\n",
    "\n",
    "# Dataset locations — the notebook first looks beside itself, then inside data/raw/ if present.\n",
    "CANCER_PRIMARY = Path(\"lncRNA_5_Cancers.csv\")\n",
    "CANCER_FALLBACK = Path(\"data/raw/lncRNA_5_Cancers.csv\")\n",
    "REG_PRIMARY = Path(\"GDSC2_13drugs.csv\")\n",
    "REG_FALLBACK = Path(\"data/raw/hw3-drug-screening-data.csv\")\n",
    "\n",
    "# Output directory mirrors the course instructions.\n",
    "OUT_DIR = Path(\"hw3_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURE_CACHE_DIR = OUT_DIR / \"feature_cache\"\n",
    "FEATURE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR = OUT_DIR / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Memory-aware knobs — conservative defaults to keep SHAP tractable on laptops.\n",
    "MAX_FEATURES_CLASSIF = 1000\n",
    "MAX_FEATURES_REGRESS = 1200\n",
    "SHAP_SAMPLES_PER_CLASS = 30\n",
    "SHAP_SAMPLES_REG = 100\n",
    "SHAP_BACKGROUND_SIZE = 256\n",
    "ENABLE_SHAP_CACHE = True\n",
    "BACKGROUND_SIZE = SHAP_BACKGROUND_SIZE  # backwards compatibility\n",
    "MAX_PARALLEL = max(1, min((os.cpu_count() or 1), 4))\n",
    "\n",
    "# Experiment orchestration — persists progress so long runs are resumable.\n",
    "EXPERIMENT_STEPS = [\n",
    "    \"Task 1: data load\",\n",
    "    \"Task 1: model comparison\",\n",
    "    \"Task 2: classifier SHAP\",\n",
    "    \"Task 3: data load\",\n",
    "    \"Task 3: regressor comparison\",\n",
    "    \"Task 4: regressor SHAP\",\n",
    "]\n",
    "\n",
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Persist experiment status, durations, and failure notes for recovery.\"\"\"\n",
    "\n",
    "    def __init__(self, steps: List[str]):\n",
    "        self.steps = steps\n",
    "        self.state_path = OUT_DIR / \"experiment_state.json\"\n",
    "        self.state = {step: {\"status\": \"pending\"} for step in steps}\n",
    "        if self.state_path.exists():\n",
    "            try:\n",
    "                loaded = json.loads(self.state_path.read_text())\n",
    "            except json.JSONDecodeError:\n",
    "                loaded = {}\n",
    "            for step, info in loaded.items():\n",
    "                if step in self.state:\n",
    "                    self.state[step].update(info)\n",
    "        completed = sum(1 for info in self.state.values() if info.get(\"status\") == \"completed\")\n",
    "        total = len(steps)\n",
    "        self.bar = tqdm(total=total, desc=\"Experiment automation\", leave=False, dynamic_ncols=True)\n",
    "        if completed:\n",
    "            self.bar.update(completed)\n",
    "        self.bar.set_postfix_str(\"Idle\")\n",
    "\n",
    "    def save(self) -> None:\n",
    "        self.state_path.write_text(json.dumps(self.state, indent=2))\n",
    "\n",
    "    def start(self, step: str) -> None:\n",
    "        info = self.state.setdefault(step, {})\n",
    "        info.update(\n",
    "            {\n",
    "                \"status\": \"running\",\n",
    "                \"started_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "        )\n",
    "        self.save()\n",
    "        self.bar.set_postfix_str(f\"Running: {step}\")\n",
    "\n",
    "    def complete(self, step: str, metadata: Optional[Dict[str, float]] = None) -> None:\n",
    "        info = self.state.setdefault(step, {})\n",
    "        prev_status = info.get(\"status\")\n",
    "        info.update(\n",
    "            {\n",
    "                \"status\": \"completed\",\n",
    "                \"completed_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "        )\n",
    "        if metadata:\n",
    "            info[\"metrics\"] = {k: float(v) for k, v in metadata.items()}\n",
    "        self.save()\n",
    "        if prev_status != \"completed\":\n",
    "            self.bar.update(1)\n",
    "        self.bar.set_postfix_str(\"Idle\")\n",
    "\n",
    "    def fail(\n",
    "        self,\n",
    "        step: str,\n",
    "        error: Exception,\n",
    "        seconds: Optional[float] = None,\n",
    "        delta_gb: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        info = self.state.setdefault(step, {})\n",
    "        payload = {\n",
    "            \"status\": \"failed\",\n",
    "            \"failed_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error\": f\"{error.__class__.__name__}: {error}\",\n",
    "        }\n",
    "        if seconds is not None:\n",
    "            payload[\"seconds\"] = float(seconds)\n",
    "        if delta_gb is not None:\n",
    "            payload[\"delta_gb\"] = float(delta_gb)\n",
    "        info.update(payload)\n",
    "        self.save()\n",
    "        self.bar.set_postfix_str(f\"Failed: {step}\")\n",
    "\n",
    "\n",
    "EXPERIMENT_TRACKER = ExperimentTracker(EXPERIMENT_STEPS)\n",
    "\n",
    "\n",
    "def checkpoint_path(label: str) -> Path:\n",
    "    \"\"\"Return the on-disk location for a checkpoint bundle.\"\"\"\n",
    "    return CHECKPOINT_DIR / f\"{label}.joblib\"\n",
    "\n",
    "\n",
    "def has_checkpoint(label: str) -> bool:\n",
    "    return checkpoint_path(label).exists()\n",
    "\n",
    "\n",
    "def load_checkpoint(label: str):\n",
    "    return load(checkpoint_path(label))\n",
    "\n",
    "\n",
    "def save_checkpoint(label: str, payload: Dict[str, object]) -> None:\n",
    "    dump(payload, checkpoint_path(label), compress=3)\n",
    "\n",
    "\n",
    "def fingerprint_columns(columns: List[str]) -> str:\n",
    "    \"\"\"Stable SHA1 fingerprint for a column ordering (ensures checkpoint compatibility).\"\"\"\n",
    "    joined = \"|\".join(columns)\n",
    "    return hashlib.sha1(joined.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def dataset_signature(path: Path) -> str:\n",
    "    \"\"\"Hash file metadata so checkpoints invalidate when upstream data changes.\"\"\"\n",
    "    stat = path.stat()\n",
    "    raw = f\"{path.resolve()}|{stat.st_size}|{stat.st_mtime_ns}\"\n",
    "    return hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def log(message: str) -> None:\n",
    "    \"\"\"Timestamped logger so notebook output resembles an experiment log.\"\"\"\n",
    "    stamp = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{stamp}] {message}\")\n",
    "\n",
    "\n",
    "PROCESS = psutil.Process(os.getpid())\n",
    "TIMINGS: List[Dict[str, float]] = []\n",
    "\n",
    "\n",
    "def current_memory_gb() -> float:\n",
    "    \"\"\"Return process RSS memory in gigabytes.\"\"\"\n",
    "    return PROCESS.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "\n",
    "def timed_step(label: str):\n",
    "    \"\"\"Decorator to time functions, update checkpoints, and log duration/memory usage.\"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if EXPERIMENT_TRACKER is not None:\n",
    "                EXPERIMENT_TRACKER.start(label)\n",
    "            start_time = perf_counter()\n",
    "            start_mem = current_memory_gb()\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "            except Exception as exc:  # noqa: BLE001 — we want to persist the failure.\n",
    "                elapsed = perf_counter() - start_time\n",
    "                end_mem = current_memory_gb()\n",
    "                if EXPERIMENT_TRACKER is not None:\n",
    "                    EXPERIMENT_TRACKER.fail(label, exc, seconds=elapsed, delta_gb=end_mem - start_mem)\n",
    "                raise\n",
    "            duration = perf_counter() - start_time\n",
    "            end_mem = current_memory_gb()\n",
    "            TIMINGS.append(\n",
    "                {\n",
    "                    \"step\": label,\n",
    "                    \"seconds\": duration,\n",
    "                    \"start_gb\": start_mem,\n",
    "                    \"end_gb\": end_mem,\n",
    "                    \"delta_gb\": end_mem - start_mem,\n",
    "                }\n",
    "            )\n",
    "            log(f\"{label} finished in {duration:.2f}s (Δmem {end_mem - start_mem:.3f} GB)\")\n",
    "            if EXPERIMENT_TRACKER is not None:\n",
    "                EXPERIMENT_TRACKER.complete(\n",
    "                    label,\n",
    "                    metadata={\"seconds\": duration, \"delta_gb\": end_mem - start_mem},\n",
    "                )\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def shap_cache_path(label: str) -> Path:\n",
    "    \"\"\"Return the cache file path for a SHAP artefact.\"\"\"\n",
    "    return FEATURE_CACHE_DIR / f\"{label}.npz\"\n",
    "\n",
    "\n",
    "def select_top_variance_features(frame: pd.DataFrame, max_features: int, cache_label: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Retain high-variance features using vectorised NumPy ops and optional caching.\"\"\"\n",
    "    if frame.shape[1] <= max_features:\n",
    "        return frame\n",
    "    cache_path: Optional[Path] = None\n",
    "    if cache_label:\n",
    "        cache_path = FEATURE_CACHE_DIR / f\"{cache_label}_{max_features}.json\"\n",
    "        if cache_path.exists():\n",
    "            columns = json.loads(cache_path.read_text())\n",
    "            columns = [col for col in columns if col in frame.columns]\n",
    "            if len(columns) == max_features:\n",
    "                log(f\"Reusing cached feature mask for {cache_label} ({max_features} columns).\")\n",
    "                return frame.loc[:, columns]\n",
    "    data = frame.to_numpy(dtype=np.float32, copy=False)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        variances = np.nanvar(data, axis=0)\n",
    "    top_idx = np.argpartition(variances, -max_features)[-max_features:]\n",
    "    top_idx = top_idx[np.argsort(variances[top_idx])[::-1]]\n",
    "    columns = frame.columns[top_idx].tolist()\n",
    "    if cache_path:\n",
    "        cache_path.write_text(json.dumps(columns))\n",
    "    return frame.loc[:, columns]\n",
    "\n",
    "\n",
    "def tree_background(sample: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Downsample a SHAP background dataset for tree explainers (linear cost w.r.t. columns).\"\"\"\n",
    "    if sample.shape[0] <= SHAP_BACKGROUND_SIZE:\n",
    "        return sample\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    idx = rng.choice(sample.shape[0], size=SHAP_BACKGROUND_SIZE, replace=False)\n",
    "    return sample[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e6002",
   "metadata": {},
   "source": [
    "## 1. Data Loading Utilities\n",
    "These loaders detect identifier/target columns, enforce numeric typing, drop all-NaN features, and apply the variance cap defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c41718f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_data_path(primary: Path, fallback: Optional[Path]) -> Optional[Path]:\n",
    "    \"\"\"Return the first available dataset path among primary and fallback hints.\"\"\"\n",
    "    for candidate in (primary, fallback):\n",
    "        if candidate is None:\n",
    "            continue\n",
    "        candidate_path = Path(candidate)\n",
    "        if candidate_path.exists():\n",
    "            return candidate_path\n",
    "    return None\n",
    "\n",
    "def detect_id_and_target(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    id_col = None\n",
    "    target_col = None\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if df[col].astype(str).str.contains(r\"^TCGA-\", na=False).any():\n",
    "            id_col = col\n",
    "            break\n",
    "    if id_col is None:\n",
    "        for col in df.columns:\n",
    "            if re.search(r\"(id|patient|sample)\", col, re.I) and df[col].nunique(dropna=True) > 10:\n",
    "                id_col = col\n",
    "                break\n",
    "    for col in df.columns:\n",
    "        values = set(map(str, df[col].dropna().unique()))\n",
    "        if values.issubset(CANCER_SET) and len(values) == len(CANCER_SET):\n",
    "            target_col = col\n",
    "            break\n",
    "    if target_col is None:\n",
    "        for col in df.columns:\n",
    "            if re.search(r\"(cancer|type|label|class)\", col, re.I):\n",
    "                target_col = col\n",
    "                break\n",
    "    return id_col, target_col\n",
    "\n",
    "\n",
    "def read_csv_efficient(path: Path, usecols: Optional[List[str]] = None, nrows: Optional[int] = None):\n",
    "    \"\"\"Fast CSV reader that prefers Polars when available.\"\"\"\n",
    "    if HAVE_POLARS:\n",
    "        try:\n",
    "            if nrows is not None:\n",
    "                df_pl = pl.read_csv(path, columns=usecols, n_rows=nrows)\n",
    "            else:\n",
    "                df_pl = pl.read_csv(path, columns=usecols, low_memory=True)\n",
    "            return df_pl.to_pandas(use_pyarrow_extension_array=False)\n",
    "        except Exception as exc:\n",
    "            log(f\"Polars read failed ({exc}); falling back to pandas.\")\n",
    "    return pd.read_csv(path, usecols=usecols, nrows=nrows)\n",
    "\n",
    "\n",
    "@timed_step(\"Task 1: data load\")\n",
    "def memory_savvy_read_cancers(csv_path: Path, max_features: int) -> Tuple[pd.DataFrame, pd.Series, Optional[pd.Series], str, Optional[str]]:\n",
    "    dataset_sig = dataset_signature(csv_path)\n",
    "    checkpoint_label = f\"task1_data_{dataset_sig}_{max_features}\"\n",
    "    if has_checkpoint(checkpoint_label):\n",
    "        cached = load_checkpoint(checkpoint_label)\n",
    "        if cached.get(\"signature\") == dataset_sig and cached.get(\"max_features\") == max_features:\n",
    "            log(\"Loaded classification dataset from checkpoint.\")\n",
    "            return (\n",
    "                cached[\"X\"],\n",
    "                cached[\"y\"],\n",
    "                cached.get(\"ids\"),\n",
    "                cached[\"target_col\"],\n",
    "                cached.get(\"id_col\"),\n",
    "            )\n",
    "\n",
    "    header_cols = read_csv_efficient(csv_path, nrows=0).columns.tolist()\n",
    "    sample_df = read_csv_efficient(csv_path, nrows=200)\n",
    "    id_col, target_col = detect_id_and_target(sample_df)\n",
    "    if target_col is None:\n",
    "        raise RuntimeError(\"Unable to detect target column in classification dataset.\")\n",
    "\n",
    "    feature_cols = [c for c in header_cols if c not in {id_col, target_col}]\n",
    "    selected_cols = feature_cols[:max_features]\n",
    "    usecols = [target_col] + ([id_col] if id_col else []) + selected_cols\n",
    "\n",
    "    df = read_csv_efficient(csv_path, usecols=usecols)\n",
    "    y = df[target_col].astype(str)\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    ids = None\n",
    "    if id_col and id_col in X.columns:\n",
    "        ids = X[id_col].astype(str)\n",
    "        X = X.drop(columns=[id_col])\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "    X = X.astype(np.float32)\n",
    "    X = X.loc[:, X.notna().any(axis=0)]\n",
    "    X = select_top_variance_features(X, max_features, cache_label=f\"{csv_path.stem}_class\")\n",
    "    result = (X, y, ids, target_col, id_col)\n",
    "    save_checkpoint(\n",
    "        checkpoint_label,\n",
    "        {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"ids\": ids,\n",
    "            \"target_col\": target_col,\n",
    "            \"id_col\": id_col,\n",
    "            \"max_features\": max_features,\n",
    "            \"signature\": dataset_sig,\n",
    "            \"columns_sig\": fingerprint_columns(list(X.columns)),\n",
    "            \"shape\": X.shape,\n",
    "        },\n",
    "    )\n",
    "    gc.collect()\n",
    "    return result\n",
    "\n",
    "\n",
    "@timed_step(\"Task 3: data load\")\n",
    "def memory_savvy_read_gdsc2(csv_path: Path, max_features: int) -> Tuple[pd.DataFrame, pd.Series, pd.Series, Dict[str, object]]:\n",
    "    dataset_sig = dataset_signature(csv_path)\n",
    "    checkpoint_label = f\"task3_data_{dataset_sig}_{max_features}\"\n",
    "    if has_checkpoint(checkpoint_label):\n",
    "        cached = load_checkpoint(checkpoint_label)\n",
    "        if cached.get(\"signature\") == dataset_sig and cached.get(\"max_features\") == max_features:\n",
    "            log(\"Loaded regression dataset from checkpoint.\")\n",
    "            return cached[\"X\"], cached[\"y\"], cached[\"keys\"], cached[\"meta\"]\n",
    "\n",
    "    header_cols = read_csv_efficient(csv_path, nrows=0).columns.tolist()\n",
    "    target_col = \"LN_IC50\"\n",
    "\n",
    "    id_cols: List[str] = []\n",
    "    for cand in [\"CELL_LINE_NAME\", \"cell_line\", \"CELL_LINE\", \"CellLine\", \"cellLine\", \"cell_line_name\"]:\n",
    "        if cand in header_cols:\n",
    "            id_cols.append(cand)\n",
    "            break\n",
    "    for cand in [\"DRUG_NAME\", \"drug_name\", \"Drug\", \"DRUG\", \"drug\"]:\n",
    "        if cand in header_cols:\n",
    "            id_cols.append(cand)\n",
    "            break\n",
    "\n",
    "    if target_col not in header_cols:\n",
    "        raise RuntimeError(\"Expected LN_IC50 column missing in regression dataset.\")\n",
    "    if not id_cols:\n",
    "        raise RuntimeError(\"Could not detect cell line / drug identifier columns.\")\n",
    "\n",
    "    feature_cols = [c for c in header_cols if c not in id_cols + [target_col]]\n",
    "    selected_cols = feature_cols[:max_features]\n",
    "    usecols = id_cols + [target_col] + selected_cols\n",
    "\n",
    "    df = read_csv_efficient(csv_path, usecols=usecols)\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\").astype(np.float32)\n",
    "\n",
    "    if len(id_cols) >= 2:\n",
    "        keys = df[id_cols[0]].astype(str) + \"|\" + df[id_cols[1]].astype(str)\n",
    "    else:\n",
    "        keys = df[id_cols[0]].astype(str)\n",
    "\n",
    "    X = df.drop(columns=id_cols + [target_col])\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "    X = X.astype(np.float32)\n",
    "    X = X.loc[:, X.notna().any(axis=0)]\n",
    "    X = select_top_variance_features(X, max_features, cache_label=f\"{csv_path.stem}_reg\")\n",
    "    meta = {\n",
    "        \"n_rows\": len(df),\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"id_cols\": id_cols,\n",
    "        \"target\": target_col,\n",
    "    }\n",
    "    result = (X, y, keys, meta)\n",
    "    save_checkpoint(\n",
    "        checkpoint_label,\n",
    "        {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"keys\": keys,\n",
    "            \"meta\": meta,\n",
    "            \"max_features\": max_features,\n",
    "            \"signature\": dataset_sig,\n",
    "            \"columns_sig\": fingerprint_columns(list(X.columns)),\n",
    "            \"shape\": X.shape,\n",
    "        },\n",
    "    )\n",
    "    gc.collect()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287f347",
   "metadata": {},
   "source": [
    "### Data Loading Optimisations\n",
    "- Prefer **Polars** for CSV ingestion when available (falls back to pandas if installation fails).\n",
    "- Added lightweight timing/memory instrumentation via `timed_step`, now covering data ingestion in addition to modelling.\n",
    "- Future runs will highlight load times in the optimisation summary table above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394979e8",
   "metadata": {},
   "source": [
    "### Feature Engineering Optimisations\n",
    "- Cached variance-based feature subsets per dataset to avoid recomputation across runs.\n",
    "- Switched variance computation to vectorised NumPy for faster execution on wide matrices.\n",
    "- Reused the Polars-backed reader from the ingestion pass to minimise conversion overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c082d",
   "metadata": {},
   "source": [
    "## 2. Modelling & SHAP Utilities\n",
    "These helpers encapsulate the repetitive parts of Tasks 1–4: training the model suites, logging metrics, and generating SHAP summaries. Artefacts are written to `hw3_outputs/` for direct inclusion in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879025a0",
   "metadata": {},
   "source": [
    "### Training Optimisations\n",
    "- Leveraged joblib with a `tqdm`-backed progress bar to evaluate models in parallel (bounded by available CPU cores).\n",
    "- Ensured all tree ensembles use multi-threaded backends (`n_jobs=-1` where supported).\n",
    "- Pipelines are re-fit on the full dataset only for the winning model, reducing redundant estimator training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7603f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed_step(\"Task 1: model comparison\")\n",
    "def train_compare_classifiers(X: pd.DataFrame, y: pd.Series, random_state: int) -> Tuple[pd.DataFrame, Pipeline, Dict[int, str]]:\n",
    "    \"\"\"Train the required classifiers with parallel execution.\"\"\"\n",
    "    class_names = sorted(y.astype(str).unique())\n",
    "    class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "    idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
    "    col_signature = fingerprint_columns(list(X.columns))\n",
    "    model_checkpoint = f\"task1_model_{col_signature}_{len(X)}\"\n",
    "\n",
    "    if has_checkpoint(model_checkpoint):\n",
    "        cached = load_checkpoint(model_checkpoint)\n",
    "        if cached.get(\"random_state\") == random_state and cached.get(\"class_names\") == class_names:\n",
    "            log(\"Loaded Task 1 model comparison from checkpoint.\")\n",
    "            metrics_df = cached[\"metrics_df\"]\n",
    "            cm_df = cached[\"confusion_df\"]\n",
    "            report_df = cached[\"report_df\"]\n",
    "            best_pipeline = cached[\"best_pipeline\"]\n",
    "            idx_to_class = cached[\"idx_to_class\"]\n",
    "            best_name = cached[\"best_model_name\"]\n",
    "            metrics_df.to_csv(OUT_DIR / \"task1_model_comparison.csv\", index=False)\n",
    "            cm_df.to_csv(OUT_DIR / \"task1_confusion_matrix.csv\")\n",
    "            report_df.to_csv(OUT_DIR / \"task1_classification_report.csv\")\n",
    "            (OUT_DIR / \"task1_best_model.txt\").write_text(str(best_name))\n",
    "            return metrics_df, best_pipeline, idx_to_class\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=random_state,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    y_train_series = pd.Series(y_train)\n",
    "    y_test_series = pd.Series(y_test)\n",
    "    y_train_encoded = y_train_series.map(class_to_idx).astype(int)\n",
    "    y_test_array = y_test_series.to_numpy()\n",
    "\n",
    "    base_estimators: Dict[str, object] = {\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=random_state, min_samples_leaf=2, class_weight=\"balanced\"),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=120, random_state=random_state, n_jobs=-1, class_weight=\"balanced_subsample\"),\n",
    "        \"GBM\": GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=3, random_state=random_state),\n",
    "    }\n",
    "    if HAVE_XGB:\n",
    "        base_estimators[\"XGBoost\"] = XGBClassifier(\n",
    "            objective=\"multi:softprob\",\n",
    "            eval_metric=\"mlogloss\",\n",
    "            n_estimators=160,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=MAX_PARALLEL,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    if HAVE_LGBM:\n",
    "        base_estimators[\"LightGBM\"] = LGBMClassifier(\n",
    "            n_estimators=160,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=63,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    if HAVE_CAT:\n",
    "        base_estimators[\"CatBoost\"] = CatBoostClassifier(\n",
    "            iterations=160,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            loss_function=\"MultiClass\",\n",
    "            random_seed=random_state,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    pipelines = {\n",
    "        name: Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\", copy=False)),\n",
    "            (\"model\", estimator),\n",
    "        ])\n",
    "        for name, estimator in base_estimators.items()\n",
    "    }\n",
    "\n",
    "    def _fit_pipeline(name: str, pipe: Pipeline):\n",
    "        local_pipe = clone(pipe)\n",
    "        local_pipe.fit(X_train, y_train_encoded.to_numpy())\n",
    "        preds_idx = local_pipe.predict(X_test)\n",
    "        preds_idx = np.asarray(preds_idx)\n",
    "        if preds_idx.ndim > 1:\n",
    "            preds_idx = preds_idx.argmax(axis=1)\n",
    "        preds_idx = preds_idx.astype(int)\n",
    "        preds_array = np.array([idx_to_class[int(i)] for i in preds_idx], dtype=object)\n",
    "        acc = accuracy_score(y_test_array, preds_array)\n",
    "        f1m = f1_score(y_test_array, preds_array, average=\"macro\")\n",
    "        return {\n",
    "            \"Model\": name,\n",
    "            \"Test_Accuracy\": acc,\n",
    "            \"Test_F1_Macro\": f1m,\n",
    "            \"preds\": preds_array,\n",
    "        }\n",
    "\n",
    "    tasks = list(pipelines.items())\n",
    "    parallelism = min(MAX_PARALLEL, len(tasks))\n",
    "\n",
    "    with tqdm_joblib(tqdm(total=len(tasks), desc=\"Task 1: classifiers\", unit=\"model\", leave=False)):\n",
    "        fitted = Parallel(n_jobs=parallelism, backend=\"loky\")(delayed(_fit_pipeline)(name, pipe) for name, pipe in tasks)\n",
    "\n",
    "    metrics_records = [{\"Model\": entry[\"Model\"], \"Test_Accuracy\": entry[\"Test_Accuracy\"], \"Test_F1_Macro\": entry[\"Test_F1_Macro\"]} for entry in fitted]\n",
    "    metrics_df = pd.DataFrame(metrics_records).sort_values([\"Test_F1_Macro\", \"Test_Accuracy\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_name = metrics_df.iloc[0][\"Model\"]\n",
    "    best_entry = next(entry for entry in fitted if entry[\"Model\"] == best_name)\n",
    "    best_preds = best_entry[\"preds\"]\n",
    "\n",
    "    cm = confusion_matrix(y_test, best_preds, labels=class_names)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"True_{c}\" for c in class_names], columns=[f\"Pred_{c}\" for c in class_names])\n",
    "    cm_df.to_csv(OUT_DIR / \"task1_confusion_matrix.csv\")\n",
    "\n",
    "    report_df = pd.DataFrame(classification_report(\n",
    "        y_test,\n",
    "        best_preds,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "        labels=class_names,\n",
    "        target_names=class_names,\n",
    "    )).T\n",
    "    report_df.to_csv(OUT_DIR / \"task1_classification_report.csv\")\n",
    "\n",
    "    metrics_df.to_csv(OUT_DIR / \"task1_model_comparison.csv\", index=False)\n",
    "    (OUT_DIR / \"task1_best_model.txt\").write_text(str(best_name))\n",
    "\n",
    "    best_pipeline = clone(pipelines[best_name])\n",
    "    y_full_encoded = pd.Series(y).map(class_to_idx).astype(int).to_numpy()\n",
    "    best_pipeline.fit(X, y_full_encoded)\n",
    "\n",
    "    save_checkpoint(\n",
    "        model_checkpoint,\n",
    "        {\n",
    "            \"metrics_df\": metrics_df,\n",
    "            \"confusion_df\": cm_df,\n",
    "            \"report_df\": report_df,\n",
    "            \"best_pipeline\": best_pipeline,\n",
    "            \"best_model_name\": best_name,\n",
    "            \"idx_to_class\": idx_to_class,\n",
    "            \"random_state\": random_state,\n",
    "            \"class_names\": class_names,\n",
    "            \"columns_sig\": col_signature,\n",
    "            \"n_rows\": len(X),\n",
    "        },\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics_df, best_pipeline, idx_to_class\n",
    "\n",
    "\n",
    "@timed_step(\"Task 3: regressor comparison\")\n",
    "def train_compare_regressors(X: pd.DataFrame, y: pd.Series, random_state: int) -> Tuple[pd.DataFrame, Pipeline]:\n",
    "    \"\"\"Train the regression suite in parallel and return metrics plus the best pipeline.\"\"\"\n",
    "    col_signature = fingerprint_columns(list(X.columns))\n",
    "    model_checkpoint = f\"task3_model_{col_signature}_{len(X)}\"\n",
    "\n",
    "    if has_checkpoint(model_checkpoint):\n",
    "        cached = load_checkpoint(model_checkpoint)\n",
    "        if cached.get(\"random_state\") == random_state:\n",
    "            log(\"Loaded Task 3 regressor comparison from checkpoint.\")\n",
    "            metrics_df = cached[\"metrics_df\"]\n",
    "            best_pipeline = cached[\"best_pipeline\"]\n",
    "            best_name = cached[\"best_model_name\"]\n",
    "            metrics_df.to_csv(OUT_DIR / \"task3_regressor_comparison.csv\", index=False)\n",
    "            (OUT_DIR / \"task3_best_model.txt\").write_text(str(best_name))\n",
    "            return metrics_df, best_pipeline\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    categorical_features = [col for col in [\"CELL_LINE_NAME\", \"DRUG_NAME\"] if col in X.columns]\n",
    "    numeric_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "    cat_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\", copy=False)),\n",
    "        (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    ])\n",
    "    num_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\", copy=False)),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"cat\", cat_transformer, categorical_features),\n",
    "        (\"num\", num_transformer, numeric_features),\n",
    "    ])\n",
    "\n",
    "    base_estimators: Dict[str, object] = {\n",
    "        \"DecisionTreeReg\": DecisionTreeRegressor(random_state=random_state, min_samples_leaf=2),\n",
    "        \"RandomForestReg\": RandomForestRegressor(n_estimators=120, random_state=random_state, n_jobs=-1),\n",
    "        \"GBMReg\": GradientBoostingRegressor(n_estimators=120, learning_rate=0.05, max_depth=3, random_state=random_state),\n",
    "    }\n",
    "    if HAVE_XGB:\n",
    "        base_estimators[\"XGBReg\"] = XGBRegressor(\n",
    "            n_estimators=160,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=MAX_PARALLEL,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    if HAVE_LGBM:\n",
    "        base_estimators[\"LGBMReg\"] = LGBMRegressor(\n",
    "            n_estimators=160,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=63,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    if HAVE_CAT:\n",
    "        base_estimators[\"CatBoostReg\"] = CatBoostRegressor(\n",
    "            iterations=160,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            loss_function=\"RMSE\",\n",
    "            random_seed=random_state,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    pipelines = {\n",
    "        name: Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"reg\", estimator),\n",
    "        ])\n",
    "        for name, estimator in base_estimators.items()\n",
    "    }\n",
    "\n",
    "    def _fit_pipeline(name: str, pipe: Pipeline):\n",
    "        local_pipe = clone(pipe)\n",
    "        local_pipe.fit(X_train, y_train)\n",
    "        preds = local_pipe.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        return {\n",
    "            \"Model\": name,\n",
    "            \"MAE\": mae,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"R2\": r2,\n",
    "        }\n",
    "\n",
    "    tasks = list(pipelines.items())\n",
    "    parallelism = min(MAX_PARALLEL, len(tasks))\n",
    "\n",
    "    with tqdm_joblib(tqdm(total=len(tasks), desc=\"Task 3: regressors\", unit=\"model\", leave=False)):\n",
    "        fitted = Parallel(n_jobs=parallelism, backend=\"loky\")(delayed(_fit_pipeline)(name, pipe) for name, pipe in tasks)\n",
    "\n",
    "    metrics_records = [{\n",
    "        \"Model\": entry[\"Model\"],\n",
    "        \"MAE\": entry[\"MAE\"],\n",
    "        \"MSE\": entry[\"MSE\"],\n",
    "        \"RMSE\": entry[\"RMSE\"],\n",
    "        \"R2\": entry[\"R2\"],\n",
    "    } for entry in fitted]\n",
    "    metrics_df = pd.DataFrame(metrics_records).sort_values([\"RMSE\", \"MAE\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    best_name = metrics_df.iloc[0][\"Model\"]\n",
    "\n",
    "    metrics_df.to_csv(OUT_DIR / \"task3_regressor_comparison.csv\", index=False)\n",
    "    (OUT_DIR / \"task3_best_model.txt\").write_text(str(best_name))\n",
    "\n",
    "    best_pipeline = clone(pipelines[best_name])\n",
    "    y_full_encoded = pd.Series(y).map(class_to_idx).astype(int).to_numpy()\n",
    "    best_pipeline.fit(X, y_full_encoded)\n",
    "\n",
    "    save_checkpoint(\n",
    "        model_checkpoint,\n",
    "        {\n",
    "            \"metrics_df\": metrics_df,\n",
    "            \"best_pipeline\": best_pipeline,\n",
    "            \"best_model_name\": best_name,\n",
    "            \"random_state\": random_state,\n",
    "            \"columns_sig\": col_signature,\n",
    "            \"n_rows\": len(X),\n",
    "        },\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics_df, best_pipeline\n",
    "\n",
    "\n",
    "@timed_step(\"Task 2: classifier SHAP\")\n",
    "def shap_task2(best_model: Pipeline, X: pd.DataFrame, y: pd.Series, sample_ids: Optional[pd.Series], patient_id: str, idx_to_class: Dict[int, str]) -> List[Tuple[str, object]]:\n",
    "    if not HAVE_SHAP:\n",
    "        raise ImportError(\"SHAP is required for Task 2. Re-run the installation cell if needed.\")\n",
    "\n",
    "    imputer = best_model.named_steps.get(\"imputer\")\n",
    "    if imputer is not None:\n",
    "        X_matrix = imputer.transform(X)\n",
    "        feature_names = list(getattr(imputer, \"feature_names_in_\", X.columns))\n",
    "    else:\n",
    "        X_matrix = X.to_numpy(dtype=np.float32, copy=False)\n",
    "        feature_names = list(X.columns)\n",
    "\n",
    "    background = tree_background(X_matrix)\n",
    "\n",
    "    label_array = y.to_numpy()\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    subset_idx: List[int] = []\n",
    "    for cancer in tqdm(sorted(CANCER_SET), desc=\"Task 2a: sampling\", leave=False):\n",
    "        class_idx = np.where(label_array == cancer)[0]\n",
    "        if class_idx.size == 0:\n",
    "            continue\n",
    "        if class_idx.size > SHAP_SAMPLES_PER_CLASS:\n",
    "            class_idx = rng.choice(class_idx, SHAP_SAMPLES_PER_CLASS, replace=False)\n",
    "        subset_idx.extend(class_idx.tolist())\n",
    "    subset_idx = np.array(sorted(set(subset_idx)), dtype=int)\n",
    "    if subset_idx.size == 0:\n",
    "        subset_idx = np.arange(min(SHAP_SAMPLES_PER_CLASS * len(CANCER_SET), X_matrix.shape[0]))\n",
    "    X_subset = X_matrix[subset_idx]\n",
    "\n",
    "    model = best_model.named_steps[\"model\"]\n",
    "    explainer = shap.TreeExplainer(model, data=background, feature_perturbation=\"tree_path_dependent\")\n",
    "\n",
    "    cache_key = f\"task2_{model.__class__.__name__.lower()}_{len(subset_idx)}_{X_matrix.shape[1]}\"\n",
    "    cache_file = shap_cache_path(cache_key)\n",
    "    shap_by_class: Optional[List[np.ndarray]] = None\n",
    "    expected_values: Optional[np.ndarray] = None\n",
    "    if ENABLE_SHAP_CACHE and cache_file.exists():\n",
    "        cache = np.load(cache_file, allow_pickle=True)\n",
    "        cached_idx = cache[\"indices\"]\n",
    "        if cached_idx.shape == subset_idx.shape and np.array_equal(cached_idx, subset_idx):\n",
    "            shap_by_class = [np.asarray(arr) for arr in cache[\"shap\"]]\n",
    "            expected_values = np.asarray(cache[\"expected\"])\n",
    "        else:\n",
    "            cache_file.unlink(missing_ok=True)\n",
    "\n",
    "    if shap_by_class is None:\n",
    "        shap_output = explainer.shap_values(X_subset)\n",
    "        if isinstance(shap_output, list):\n",
    "            shap_by_class = [np.asarray(arr) for arr in shap_output]\n",
    "        else:\n",
    "            if shap_output.ndim == 3:\n",
    "                shap_by_class = [np.asarray(shap_output[:, :, i]) for i in range(shap_output.shape[2])]\n",
    "            else:\n",
    "                shap_by_class = [np.asarray(shap_output)]\n",
    "        expected_values = np.asarray(explainer.expected_value)\n",
    "        if ENABLE_SHAP_CACHE:\n",
    "            np.savez_compressed(cache_file, shap=np.array(shap_by_class, dtype=object), expected=expected_values, indices=subset_idx)\n",
    "    else:\n",
    "        expected_values = np.asarray(explainer.expected_value)\n",
    "\n",
    "    records = []\n",
    "    classes = list(model.classes_) if hasattr(model, \"classes_\") else list(range(len(shap_by_class)))\n",
    "\n",
    "    def _label_name(class_value):\n",
    "        if isinstance(class_value, (np.integer, int)):\n",
    "            return idx_to_class.get(int(class_value), str(class_value))\n",
    "        return str(class_value)\n",
    "\n",
    "    for class_index, class_name in enumerate(tqdm(classes, desc=\"Task 2a: aggregation\", leave=False)):\n",
    "        shap_matrix = np.asarray(shap_by_class[class_index])\n",
    "        mean_abs = np.abs(shap_matrix).mean(axis=0)\n",
    "        top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "        label_name = _label_name(class_name)\n",
    "        for rank, feat_idx in enumerate(top_idx, start=1):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"CancerType\": label_name,\n",
    "                    \"Rank\": rank,\n",
    "                    \"Feature\": feature_names[feat_idx],\n",
    "                    \"Mean|SHAP|\": float(mean_abs[feat_idx]),\n",
    "                }\n",
    "            )\n",
    "    pd.DataFrame(records).to_csv(OUT_DIR / \"task2a_top10_features_per_cancer.csv\", index=False)\n",
    "\n",
    "    shap.initjs()\n",
    "    if sample_ids is not None and sample_ids.notna().any():\n",
    "        matches = sample_ids[sample_ids.astype(str) == patient_id]\n",
    "        patient_position = matches.index[0] if not matches.empty else 0\n",
    "    else:\n",
    "        patient_position = 0\n",
    "    row_matrix = X_matrix[[patient_position]]\n",
    "\n",
    "    shap_row = explainer.shap_values(row_matrix)\n",
    "    if isinstance(shap_row, list):\n",
    "        shap_rows = [np.asarray(arr) for arr in shap_row]\n",
    "    else:\n",
    "        if shap_row.ndim == 3:\n",
    "            shap_rows = [np.asarray(shap_row[:, :, i]) for i in range(shap_row.shape[2])]\n",
    "        else:\n",
    "            shap_rows = [np.asarray(shap_row)]\n",
    "\n",
    "    inline_plots: List[Tuple[str, object]] = []\n",
    "    expected_array = np.asarray(explainer.expected_value)\n",
    "    for class_index, class_name in enumerate(tqdm(classes, desc=\"Task 2b: force plots\", leave=False)):\n",
    "        expected = expected_array[class_index] if expected_array.ndim > 0 else expected_array\n",
    "        label_name = _label_name(class_name)\n",
    "        force_plot = shap.force_plot(expected, shap_rows[class_index][0, :], row_matrix[0, :], feature_names=feature_names, matplotlib=False)\n",
    "        shap.save_html(str(OUT_DIR / f\"task2b_forceplot_{label_name}_patient_{patient_id.replace(':','-')}.html\"), force_plot)\n",
    "        inline_plots.append((label_name, force_plot))\n",
    "\n",
    "    return inline_plots\n",
    "\n",
    "\n",
    "@timed_step(\"Task 4: regressor SHAP\")\n",
    "def shap_task4(best_reg_model: Pipeline, X: pd.DataFrame, y: pd.Series, keys: pd.Series) -> None:\n",
    "    if not HAVE_SHAP:\n",
    "        raise ImportError(\"SHAP is required for Task 4. Re-run the installation cell if needed.\")\n",
    "\n",
    "    preprocessor = best_reg_model.named_steps.get(\"preprocessor\")\n",
    "    if preprocessor is not None:\n",
    "        X_matrix = preprocessor.transform(X)\n",
    "        if hasattr(X_matrix, \"toarray\"):\n",
    "            X_matrix = X_matrix.toarray()\n",
    "        feature_names = list(preprocessor.get_feature_names_out())\n",
    "    else:\n",
    "        X_matrix = X.to_numpy(dtype=np.float32, copy=False)\n",
    "        feature_names = list(X.columns)\n",
    "\n",
    "    background = tree_background(X_matrix)\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    sample_idx = rng.choice(X_matrix.shape[0], size=min(SHAP_SAMPLES_REG, X_matrix.shape[0]), replace=False)\n",
    "    X_subset = X_matrix[sample_idx]\n",
    "    drugs_subset = keys.iloc[sample_idx]\n",
    "\n",
    "    model = best_reg_model.named_steps[\"reg\"]\n",
    "    explainer = shap.TreeExplainer(model, data=background, feature_perturbation=\"tree_path_dependent\")\n",
    "\n",
    "    cache_key = f\"task4_{model.__class__.__name__.lower()}_{len(sample_idx)}_{X_matrix.shape[1]}\"\n",
    "    cache_file = shap_cache_path(cache_key)\n",
    "    shap_matrix: Optional[np.ndarray] = None\n",
    "    if ENABLE_SHAP_CACHE and cache_file.exists():\n",
    "        cache = np.load(cache_file, allow_pickle=True)\n",
    "        cached_idx = cache[\"indices\"]\n",
    "        if cached_idx.shape == sample_idx.shape and np.array_equal(cached_idx, sample_idx):\n",
    "            shap_matrix = np.asarray(cache[\"shap\"])\n",
    "        else:\n",
    "            cache_file.unlink(missing_ok=True)\n",
    "\n",
    "    if shap_matrix is None:\n",
    "        shap_output = explainer.shap_values(X_subset)\n",
    "        if isinstance(shap_output, list):\n",
    "            shap_matrix = np.asarray(shap_output[0])\n",
    "        else:\n",
    "            shap_matrix = np.asarray(shap_output)\n",
    "        if ENABLE_SHAP_CACHE:\n",
    "            np.savez_compressed(cache_file, shap=shap_matrix, indices=sample_idx)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    records = []\n",
    "    for drug in tqdm(sorted(drugs_subset.unique()), desc=\"Task 4a: per-drug SHAP\", leave=False):\n",
    "        mask = (drugs_subset == drug).values\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        mean_abs = np.abs(shap_matrix[mask]).mean(axis=0)\n",
    "        top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "        for rank, feat_idx in enumerate(top_idx, start=1):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"Drug\": drug,\n",
    "                    \"Rank\": rank,\n",
    "                    \"Feature\": feature_names[feat_idx],\n",
    "                    \"Mean|SHAP|\": float(mean_abs[feat_idx]),\n",
    "                }\n",
    "            )\n",
    "    pd.DataFrame(records).to_csv(OUT_DIR / \"task4a_top10_features_per_drug.csv\", index=False)\n",
    "\n",
    "    preds = best_reg_model.predict(X)\n",
    "    errors = np.abs(preds - y.values)\n",
    "    idx_min = int(np.argmin(errors))\n",
    "    least_key = keys.iloc[idx_min]\n",
    "\n",
    "    row_matrix = X_matrix[[idx_min]]\n",
    "    shap_row = explainer.shap_values(row_matrix)\n",
    "    if isinstance(shap_row, list):\n",
    "        shap_row = np.asarray(shap_row[0])\n",
    "    else:\n",
    "        shap_row = np.asarray(shap_row)\n",
    "\n",
    "    mean_abs = np.abs(shap_row[0, :])\n",
    "    top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"Rank\": np.arange(1, 11),\n",
    "            \"Feature\": [feature_names[i] for i in top_idx],\n",
    "            \"Absolute_SHAP\": mean_abs[top_idx].astype(float),\n",
    "        }\n",
    "    ).to_csv(OUT_DIR / f\"task4b_top10_features_least_error_{least_key.replace('|', '_')}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ea10e",
   "metadata": {},
   "source": [
    "## 3. Task 1 — Classification Dataset Reconnaissance\n",
    "We load the lncRNA expression matrix with the memory-savvy routine. The summary table captures the sample size, retained feature count, and identifier columns for citation in the written report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60ece10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment automation:  33%|███▎      | 2/6 [00:00<00:00, 17.28it/s, Idle]                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:30:32] Loaded classification dataset from checkpoint.\n",
      "[15:30:32] Task 1: data load finished in 0.06s (Δmem 0.002 GB)\n",
      "[15:30:32] Task 1 dataset loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>selected_features</th>\n",
       "      <th>target_column</th>\n",
       "      <th>id_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2529</td>\n",
       "      <td>1000</td>\n",
       "      <td>Class</td>\n",
       "      <td>Ensembl_ID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rows  selected_features target_column   id_column\n",
       "0  2529               1000         Class  Ensembl_ID"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000005206.15</th>\n",
       "      <th>ENSG00000083622.8</th>\n",
       "      <th>ENSG00000088970.14</th>\n",
       "      <th>ENSG00000099869.7</th>\n",
       "      <th>ENSG00000100181.20</th>\n",
       "      <th>ENSG00000104691.13</th>\n",
       "      <th>ENSG00000115934.11</th>\n",
       "      <th>ENSG00000117242.7</th>\n",
       "      <th>ENSG00000118412.11</th>\n",
       "      <th>ENSG00000122043.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.390813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.918266</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>0.341984</td>\n",
       "      <td>2.194036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.569750</td>\n",
       "      <td>1.159419</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.144547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.961410</td>\n",
       "      <td>0.047186</td>\n",
       "      <td>1.677598</td>\n",
       "      <td>2.605298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.180583</td>\n",
       "      <td>1.127571</td>\n",
       "      <td>0.131274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.484817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.896470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087972</td>\n",
       "      <td>3.176764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.690582</td>\n",
       "      <td>1.161923</td>\n",
       "      <td>0.109720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.789058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.439171</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>0.502293</td>\n",
       "      <td>2.679842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.659525</td>\n",
       "      <td>1.463067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.258763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.941660</td>\n",
       "      <td>0.050283</td>\n",
       "      <td>0.098625</td>\n",
       "      <td>2.841588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.296678</td>\n",
       "      <td>1.728514</td>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000005206.15  ENSG00000083622.8  ENSG00000088970.14  \\\n",
       "0            3.390813                0.0            2.918266   \n",
       "1            3.144547                0.0            1.961410   \n",
       "2            2.484817                0.0            2.896470   \n",
       "3            2.789058                0.0            2.439171   \n",
       "4            3.258763                0.0            1.941660   \n",
       "\n",
       "   ENSG00000099869.7  ENSG00000100181.20  ENSG00000104691.13  \\\n",
       "0           0.014832            0.341984            2.194036   \n",
       "1           0.047186            1.677598            2.605298   \n",
       "2           0.000000            0.087972            3.176764   \n",
       "3           0.022316            0.502293            2.679842   \n",
       "4           0.050283            0.098625            2.841588   \n",
       "\n",
       "   ENSG00000115934.11  ENSG00000117242.7  ENSG00000118412.11  \\\n",
       "0                 0.0           1.569750            1.159419   \n",
       "1                 0.0           1.180583            1.127571   \n",
       "2                 0.0           1.690582            1.161923   \n",
       "3                 0.0           1.659525            1.463067   \n",
       "4                 0.0           1.296678            1.728514   \n",
       "\n",
       "   ENSG00000122043.9  \n",
       "0           0.028200  \n",
       "1           0.131274  \n",
       "2           0.109720  \n",
       "3           0.000000  \n",
       "4           0.019417  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_path = resolve_data_path(CANCER_PRIMARY, CANCER_FALLBACK)\n",
    "assert cancer_path is not None, \"Classification CSV missing – please place lncRNA_5_Cancers.csv alongside the notebook.\"\n",
    "\n",
    "Xc, yc, sample_ids, class_col, id_col = memory_savvy_read_cancers(cancer_path, MAX_FEATURES_CLASSIF)\n",
    "\n",
    "summary_cls = pd.DataFrame(\n",
    "    {\n",
    "        \"rows\": [len(Xc)],\n",
    "        \"selected_features\": [Xc.shape[1]],\n",
    "        \"target_column\": [class_col],\n",
    "        \"id_column\": [id_col],\n",
    "    }\n",
    ")\n",
    "\n",
    "log(\"Task 1 dataset loaded.\")\n",
    "display(summary_cls)\n",
    "Xc.iloc[:5, :10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83f5e4",
   "metadata": {},
   "source": [
    "## 4. Task 1 — Model Comparison\n",
    "We benchmark the required classifiers under a shared preprocessing pipeline (median imputation). Macro-F1 is the primary ranking metric to respect class balance across the five cancers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "615d8389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment automation:  33%|███▎      | 2/6 [00:00<00:00, 17.28it/s, Running: Task 1: model comparison]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 199047\n",
      "[LightGBM] [Info] Number of data points in the train set: 2023, number of used features: 979\n",
      "[LightGBM] [Info] Start training from score -1.567332\n",
      "[LightGBM] [Info] Start training from score -1.601070\n",
      "[LightGBM] [Info] Start training from score -1.625885\n",
      "[LightGBM] [Info] Start training from score -1.635986\n",
      "[LightGBM] [Info] Start training from score -1.618375\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment automation:  33%|███▎      | 2/6 [00:12<00:00, 17.28it/s, Running: Task 1: model comparison]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrmendez/.transformerlab/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "Experiment automation:  50%|█████     | 3/6 [02:49<03:31, 70.65s/it, Idle]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:22] Task 1: model comparison finished in 169.44s (Δmem -0.179 GB)\n",
      "[15:33:22] Task 1 model sweep complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_F1_Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.974308</td>\n",
       "      <td>0.974226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.972332</td>\n",
       "      <td>0.972386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.970356</td>\n",
       "      <td>0.970409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBM</td>\n",
       "      <td>0.960474</td>\n",
       "      <td>0.960552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.930830</td>\n",
       "      <td>0.930806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.207510</td>\n",
       "      <td>0.068740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Test_Accuracy  Test_F1_Macro\n",
       "0  RandomForest       0.974308       0.974226\n",
       "1      LightGBM       0.972332       0.972386\n",
       "2       XGBoost       0.970356       0.970409\n",
       "3           GBM       0.960474       0.960552\n",
       "4  DecisionTree       0.930830       0.930806\n",
       "5      CatBoost       0.207510       0.068740"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_results, best_classifier, idx_to_class = train_compare_classifiers(Xc, yc, RANDOM_STATE)\n",
    "\n",
    "log(\"Task 1 model sweep complete.\")\n",
    "cls_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025956d",
   "metadata": {},
   "source": [
    "## 5. Task 1 — Confusion Matrix & Per-Class Report\n",
    "The confusion matrix and class-wise precision/recall/F1 are required in the homework write-up. They are exported to `hw3_outputs/` and displayed here for quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25841b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:22] Task 1 evaluation artefacts loaded from hw3_outputs/.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_KIRC</th>\n",
       "      <th>Pred_LUAD</th>\n",
       "      <th>Pred_LUSC</th>\n",
       "      <th>Pred_PRAD</th>\n",
       "      <th>Pred_THCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True_KIRC</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_LUAD</th>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_LUSC</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_PRAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_THCA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pred_KIRC  Pred_LUAD  Pred_LUSC  Pred_PRAD  Pred_THCA\n",
       "True_KIRC        105          0          0          0          0\n",
       "True_LUAD          0         98          4          0          0\n",
       "True_LUSC          0          9         91          0          0\n",
       "True_PRAD          0          0          0         99          0\n",
       "True_THCA          0          0          0          0        100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KIRC</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUAD</th>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUSC</th>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRAD</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THCA</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.974308</td>\n",
       "      <td>0.974308</td>\n",
       "      <td>0.974308</td>\n",
       "      <td>0.974308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.974757</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.974226</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.974723</td>\n",
       "      <td>0.974308</td>\n",
       "      <td>0.974286</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "KIRC           1.000000  1.000000  1.000000  105.000000\n",
       "LUAD           0.915888  0.960784  0.937799  102.000000\n",
       "LUSC           0.957895  0.910000  0.933333  100.000000\n",
       "PRAD           1.000000  1.000000  1.000000   99.000000\n",
       "THCA           1.000000  1.000000  1.000000  100.000000\n",
       "accuracy       0.974308  0.974308  0.974308    0.974308\n",
       "macro avg      0.974757  0.974157  0.974226  506.000000\n",
       "weighted avg   0.974723  0.974308  0.974286  506.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion = pd.read_csv(OUT_DIR / \"task1_confusion_matrix.csv\", index_col=0)\n",
    "classification_report_df = pd.read_csv(OUT_DIR / \"task1_classification_report.csv\", index_col=0)\n",
    "\n",
    "log(\"Task 1 evaluation artefacts loaded from hw3_outputs/.\")\n",
    "display(confusion)\n",
    "classification_report_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167707c7",
   "metadata": {},
   "source": [
    "## 6. Task 2 — SHAP on Best Classifier\n",
    "The top-ranked classifier becomes the subject of SHAP analysis. We compute per-cancer top-10 genes and generate patient-level force plots for `TCGA-39-5011-01A` as mandated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be288b",
   "metadata": {},
   "source": [
    "### SHAP Optimisations\n",
    "- Sampled per-class subsets deterministically and cached SHAP tensors to avoid recomputation across runs.\n",
    "- Limited background size to a compact set (256 rows) and reused the TreeExplainer for patient-level explanations.\n",
    "- Added progress bars for SHAP aggregation steps so long-running tasks expose real-time feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9596004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment automation:  50%|█████     | 3/6 [02:49<03:31, 70.65s/it, Running: Task 2: classifier SHAP]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:22] Task 2 interprets the RandomForest.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment automation:  50%|█████     | 3/6 [02:49<03:31, 70.65s/it, Failed: Task 2: classifier SHAP] "
     ]
    },
    {
     "ename": "ExplainerError",
     "evalue": "The background dataset you provided does not cover all the leaves in the model, so TreeExplainer cannot run with the feature_perturbation=\"tree_path_dependent\" option! Try providing a larger background dataset, no background dataset, or using feature_perturbation=\"interventional\".",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mExplainerError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m best_classifier_name = cls_results.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask 2 interprets the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_classifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inline_force_plots = \u001b[43mshap_task2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENT_ID_TO_PLOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_to_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m task2_top = pd.read_csv(OUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mtask2a_top10_features_per_cancer.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m log(\u001b[33m\"\u001b[39m\u001b[33mTask 2 SHAP tables saved to hw3_outputs/.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 257\u001b[39m, in \u001b[36mtimed_step.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m start_mem = current_memory_gb()\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001 — we want to persist the failure.\u001b[39;00m\n\u001b[32m    259\u001b[39m     elapsed = perf_counter() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 344\u001b[39m, in \u001b[36mshap_task2\u001b[39m\u001b[34m(best_model, X, y, sample_ids, patient_id, idx_to_class)\u001b[39m\n\u001b[32m    341\u001b[39m         cache_file.unlink(missing_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shap_by_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     shap_output = \u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shap_output, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    346\u001b[39m         shap_by_class = [np.asarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m shap_output]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.transformerlab/miniconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:620\u001b[39m, in \u001b[36mTreeExplainer.shap_values\u001b[39m\u001b[34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[39m\n\u001b[32m    617\u001b[39m             out = np.stack(out, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m    618\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m X, y, X_missing, flat_output, tree_limit, check_additivity = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m transform = \u001b[38;5;28mself\u001b[39m.model.get_transform()\n\u001b[32m    624\u001b[39m _xgboost_cat_unsupported(\u001b[38;5;28mself\u001b[39m.model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.transformerlab/miniconda3/lib/python3.12/site-packages/shap/explainers/_tree.py:478\u001b[39m, in \u001b[36mTreeExplainer._validate_inputs\u001b[39m\u001b[34m(self, X, y, tree_limit, check_additivity)\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.fully_defined_weighting:\n\u001b[32m    469\u001b[39m         emsg = (\n\u001b[32m    470\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe background dataset you provided does \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnot cover all the leaves in the model, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    476\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mfeature_perturbation=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minterventional\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    477\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ExplainerError(emsg)\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_additivity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model_type == \u001b[33m\"\u001b[39m\u001b[33mpyspark\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    481\u001b[39m     warnings.warn(\n\u001b[32m    482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcheck_additivity requires us to run predictions which is not supported with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspark, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mignoring.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Set check_additivity=False to remove this warning\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m     )\n",
      "\u001b[31mExplainerError\u001b[39m: The background dataset you provided does not cover all the leaves in the model, so TreeExplainer cannot run with the feature_perturbation=\"tree_path_dependent\" option! Try providing a larger background dataset, no background dataset, or using feature_perturbation=\"interventional\"."
     ]
    }
   ],
   "source": [
    "best_classifier_name = cls_results.iloc[0][\"Model\"]\n",
    "log(f\"Task 2 interprets the {best_classifier_name}.\")\n",
    "inline_force_plots = shap_task2(best_classifier, Xc, yc, sample_ids, PATIENT_ID_TO_PLOT, idx_to_class)\n",
    "\n",
    "task2_top = pd.read_csv(OUT_DIR / \"task2a_top10_features_per_cancer.csv\")\n",
    "log(\"Task 2 SHAP tables saved to hw3_outputs/.\")\n",
    "task2_top.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render cached/returned force plots inline for quick review\n",
    "for cancer_type, force_plot in inline_force_plots:\n",
    "    log(f\"Force plot for {cancer_type}\")\n",
    "    display(force_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622313d",
   "metadata": {},
   "source": [
    "The five force plots above are also saved as HTML files in `hw3_outputs/task2b_forceplot_<Cancer>_patient_TCGA-39-5011-01A.html` for sharing or screenshot capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e195d6",
   "metadata": {},
   "source": [
    "## 7. Task 3 — Regression Dataset Reconnaissance\n",
    "We repeat the data audit for the GDSC2 drug-response table, capturing dimensionality and ID columns to justify preprocessing choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208894f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_path = resolve_data_path(REG_PRIMARY, REG_FALLBACK)\n",
    "assert regression_path is not None, \"Regression CSV missing – please place hw3-drug-screening-data.csv (renamed to GDSC2_13drugs.csv) alongside the notebook.\"\n",
    "\n",
    "Xr, yr, keys, meta = memory_savvy_read_gdsc2(regression_path, MAX_FEATURES_REGRESS)\n",
    "\n",
    "summary_reg = pd.DataFrame(\n",
    "    {\n",
    "        \"rows\": [meta[\"n_rows\"]],\n",
    "        \"selected_features\": [meta[\"n_features\"]],\n",
    "        \"target_column\": [meta[\"target\"]],\n",
    "        \"id_columns\": [\" & \".join(meta[\"id_cols\"])],\n",
    "    }\n",
    ")\n",
    "\n",
    "log(\"Task 3 dataset loaded.\")\n",
    "display(summary_reg)\n",
    "Xr.iloc[:5, :10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ba23c",
   "metadata": {},
   "source": [
    "## 8. Task 3 — Regressor Comparison\n",
    "With preprocessing aligned to the classification case, we evaluate the regression ensemble and rank models using RMSE (primary) plus MAE/MSE/R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98259c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_results, best_regressor = train_compare_regressors(Xr, yr, RANDOM_STATE)\n",
    "\n",
    "log(\"Task 3 model sweep complete.\")\n",
    "reg_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ddc06",
   "metadata": {},
   "source": [
    "## 9. Task 4 — SHAP on Best Regressor\n",
    "We apply SHAP to the top regressor to fulfil Tasks 4a and 4b: per-drug feature rankings and the least-error drug–cell-line explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_name = reg_results.iloc[0][\"Model\"]\n",
    "log(f\"Task 4 interprets the {best_regressor_name}.\")\n",
    "shap_task4(best_regressor, Xr, yr, keys)\n",
    "\n",
    "per_drug = pd.read_csv(OUT_DIR / \"task4a_top10_features_per_drug.csv\")\n",
    "least_error_path = sorted(OUT_DIR.glob(\"task4b_top10_features_least_error_*.csv\"))[-1]\n",
    "least_error = pd.read_csv(least_error_path)\n",
    "\n",
    "(per_drug.head(20), least_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5117d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise recorded runtimes and memory usage\n",
    "if TIMINGS:\n",
    "    timings_df = pd.DataFrame(TIMINGS)\n",
    "    display(timings_df)\n",
    "else:\n",
    "    log(\"No timings captured yet. Re-run the notebook from the beginning.\")\n",
    "\n",
    "if 'EXPERIMENT_TRACKER' in globals() and EXPERIMENT_TRACKER is not None:\n",
    "    tracker_df = pd.DataFrame(EXPERIMENT_TRACKER.state).T\n",
    "    display(tracker_df)\n",
    "    EXPERIMENT_TRACKER.bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1935a",
   "metadata": {},
   "source": [
    "## 10. Conclusion & Artefact Checklist\n",
    "- All tables/figures required by the assignment are in `hw3_outputs/`.\n",
    "- Rerun with different feature caps or SHAP sample sizes by adjusting the configuration cell at the top.\n",
    "- A natural extension is hyper-parameter tuning around the winning models or annotating the highlighted genes/drugs with biological context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
