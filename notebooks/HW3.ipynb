{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HW3 — CAP5610 Fall 2025\n",
    "# Tree-based ML for Classification/Regression + SHAP\n",
    "# Author: <Your Last Name Here>\n",
    "# File name convention: HW3_lastName  (adjust the file names before submission)\n",
    "#\n",
    "# Tasks (from assignment PDF):\n",
    "#  Task 1: Best tree-based classifier among: DecisionTree, RandomForest, GBM, XGBoost, LightGBM, CatBoost.\n",
    "#          Metrics: Accuracy, F1.\n",
    "#  Task 2: SHAP on best classifier: (a) per-cancer top-10 features, (b) force plots for ID=TCGA-39-5011-01A across 5 cancer types.\n",
    "#  Task 3: Best tree-based regressor among the same algorithms on GDSC2 13 drugs dataset (LN_IC50 target).\n",
    "#          Metrics: MAE, MSE, RMSE, R2.\n",
    "#  Task 4: SHAP on best regressor: (a) per-drug top-10 features, (b) top-10 features for the least-error drug–cell-line pair.\n",
    "#\n",
    "# NOTE: This script is optimized for memory. For very wide matrices (many genes), set MAX_FEATURES to a safe cap.\n",
    "#       For XGBoost/LightGBM/CatBoost you need those packages installed in your environment.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "                             mean_absolute_error, mean_squared_error, r2_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Optional libs (install if missing):\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    HAVE_XGB = True\n",
    "except Exception:\n",
    "    HAVE_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    HAVE_LGBM = True\n",
    "except Exception:\n",
    "    HAVE_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    HAVE_CAT = True\n",
    "except Exception:\n",
    "    HAVE_CAT = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAVE_SHAP = True\n",
    "except Exception:\n",
    "    HAVE_SHAP = False\n",
    "\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "RANDOM_STATE = 42\n",
    "CANCER_SET = {\"KIRC\",\"LUAD\",\"LUSC\",\"PRAD\",\"THCA\"}\n",
    "PATIENT_ID_TO_PLOT = \"TCGA-39-5011-01A\"\n",
    "CANCER_CSV = \"lncRNA_5_Cancers.csv\"        # Put the file alongside this script/notebook or change to absolute path\n",
    "GDSC2_CSV   = \"GDSC2_13drugs.csv\"          # Provide locally (download from the course Module 2 / link)\n",
    "OUT_DIR = Path(\"hw3_outputs\")               # Created if missing\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Memory safety knobs\n",
    "MAX_FEATURES_CLASSIF = 4000   # cap feature count (columns) to avoid OOM on laptops\n",
    "MAX_FEATURES_REGRESS = 4000\n",
    "SHAP_SAMPLES_PER_CLASS = 50   # number of rows per cancer for SHAP mean|SHAP| aggregation (Task 2a)\n",
    "SHAP_SAMPLES_REG = 200        # number of rows for regressor SHAP aggregation (Task 4a)\n",
    "\n",
    "# -------------------- UTILITIES --------------------\n",
    "def detect_id_and_target(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Detect a TCGA-like ID column and a 5-class cancer target column from a small sample.\"\"\"\n",
    "    id_col = None\n",
    "    target_col = None\n",
    "    for c in df.select_dtypes(include=['object']).columns:\n",
    "        if df[c].astype(str).str.contains(r\"^TCGA-\", na=False).any():\n",
    "            id_col = c\n",
    "            break\n",
    "    if id_col is None:\n",
    "        for c in df.columns:\n",
    "            if re.search(r\"(id|patient|sample)\", c, re.I):\n",
    "                if df[c].nunique(dropna=True) > 10:\n",
    "                    id_col = c\n",
    "                    break\n",
    "    for c in df.columns:\n",
    "        vals = set(map(str, df[c].dropna().unique()))\n",
    "        if vals.issubset(CANCER_SET) and len(vals) == 5:\n",
    "            target_col = c\n",
    "            break\n",
    "    if target_col is None:\n",
    "        for c in df.columns:\n",
    "            if re.search(r\"(cancer|type|label|class)\", c, re.I):\n",
    "                target_col = c\n",
    "                break\n",
    "    return id_col, target_col\n",
    "\n",
    "\n",
    "def memory_savvy_read_cancers(csv_path: str, max_features: int) -> Tuple[pd.DataFrame, pd.Series, Optional[pd.Series], str, Optional[str]]:\n",
    "    \"\"\"Read only header and a tiny sample to detect id/target; then load a capped subset of features.\"\"\"\n",
    "    header_cols = pd.read_csv(csv_path, nrows=0).columns.tolist()\n",
    "    sample_df = pd.read_csv(csv_path, nrows=200)\n",
    "    id_col, target_col = detect_id_and_target(sample_df)\n",
    "    if target_col is None:\n",
    "        raise RuntimeError(\"Could not detect the target column from sample.\")\n",
    "\n",
    "    feature_cols = [c for c in header_cols if c not in {id_col, target_col}]\n",
    "    selected_feature_cols = feature_cols[:max_features]\n",
    "    usecols = [target_col] + ([id_col] if id_col else []) + selected_feature_cols\n",
    "\n",
    "    df_part = pd.read_csv(csv_path, usecols=usecols)\n",
    "    y = df_part[target_col].astype(str)\n",
    "    X = df_part.drop(columns=[target_col])\n",
    "\n",
    "    ids = None\n",
    "    if id_col and id_col in X.columns:\n",
    "        ids = X[id_col].astype(str)\n",
    "        X = X.drop(columns=[id_col])\n",
    "\n",
    "    # Coerce to numeric and drop all-NaN columns\n",
    "    for c in X.columns:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.astype(np.float32)\n",
    "    X = X.loc[:, X.notna().any(axis=0)]\n",
    "    return X, y, ids, target_col, id_col\n",
    "\n",
    "\n",
    "def train_compare_classifiers(X: pd.DataFrame, y: pd.Series, random_state=RANDOM_STATE) -> Tuple[pd.DataFrame, Dict[str, Pipeline]]:\n",
    "    \"\"\"Train all required classifiers with efficient defaults; return test metrics and fitted models.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer([(\"num\", SimpleImputer(strategy=\"median\"), X.columns)], remainder=\"drop\")\n",
    "    models = {}\n",
    "\n",
    "    models[\"DecisionTree\"] = Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", DecisionTreeClassifier(random_state=random_state, min_samples_leaf=2, class_weight=\"balanced\"))\n",
    "    ])\n",
    "\n",
    "    models[\"RandomForest\"] = Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1, class_weight=\"balanced_subsample\"))\n",
    "    ])\n",
    "\n",
    "    models[\"GBM\"] = Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    if HAVE_XGB:\n",
    "        models[\"XGBoost\"] = Pipeline([\n",
    "            (\"prep\", preprocess),\n",
    "            (\"clf\", XGBClassifier(\n",
    "                objective=\"multi:softprob\", eval_metric=\"mlogloss\",\n",
    "                n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "                subsample=0.8, colsample_bytree=0.8, tree_method=\"hist\", n_jobs=-1, random_state=random_state\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "    if HAVE_LGBM:\n",
    "        models[\"LightGBM\"] = Pipeline([\n",
    "            (\"prep\", preprocess),\n",
    "            (\"clf\", LGBMClassifier(n_estimators=700, learning_rate=0.05, num_leaves=63, subsample=0.8, colsample_bytree=0.8, random_state=random_state))\n",
    "        ])\n",
    "\n",
    "    if HAVE_CAT:\n",
    "        models[\"CatBoost\"] = Pipeline([\n",
    "            (\"prep\", preprocess),\n",
    "            (\"clf\", CatBoostClassifier(iterations=600, learning_rate=0.05, depth=6, loss_function=\"MultiClass\", random_seed=random_state, verbose=False))\n",
    "        ])\n",
    "\n",
    "    rows = []\n",
    "    fitted = {}\n",
    "    for name, pipe in models.items():\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        rows.append({\"Model\": name, \"Test_Accuracy\": acc, \"Test_F1_Macro\": f1m})\n",
    "        fitted[name] = pipe\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values(by=[\"Test_F1_Macro\",\"Test_Accuracy\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Confusion matrix of best\n",
    "    best_name = res.iloc[0][\"Model\"]\n",
    "    best_model = fitted[best_name]\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_best, labels=sorted(CANCER_SET))\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"True_{c}\" for c in sorted(CANCER_SET)],\n",
    "                            columns=[f\"Pred_{c}\" for c in sorted(CANCER_SET)])\n",
    "    cm_df.to_csv(OUT_DIR/\"task1_confusion_matrix.csv\", index=True)\n",
    "    pd.DataFrame(classification_report(y_test, y_pred_best, output_dict=True, zero_division=0)).T.to_csv(OUT_DIR/\"task1_classification_report.csv\")\n",
    "\n",
    "    res.to_csv(OUT_DIR/\"task1_model_comparison.csv\", index=False)\n",
    "    with open(OUT_DIR/\"task1_best_model.txt\",\"w\") as f:\n",
    "        f.write(str(best_name))\n",
    "\n",
    "    return res, fitted\n",
    "\n",
    "\n",
    "def shap_task2(best_model: Pipeline, X: pd.DataFrame, y: pd.Series, id_series: Optional[pd.Series], patient_id: str):\n",
    "    if not HAVE_SHAP:\n",
    "        print(\"SHAP not available — install `shap` to run Task 2.\")\n",
    "        return\n",
    "    estimator = best_model.named_steps[list(best_model.named_steps.keys())[-1]]\n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "\n",
    "    # (a) Top-10 per cancer\n",
    "    df_full = X.copy()\n",
    "    df_full[\"__y__\"] = y.values\n",
    "    idxs = []\n",
    "    for cls in sorted(CANCER_SET):\n",
    "        cls_idx = df_full.index[df_full[\"__y__\"] == cls].tolist()\n",
    "        if len(cls_idx) > SHAP_SAMPLES_PER_CLASS:\n",
    "            rng = np.random.RandomState(RANDOM_STATE)\n",
    "            cls_idx = list(rng.choice(cls_idx, SHAP_SAMPLES_PER_CLASS, replace=False))\n",
    "        idxs.extend(cls_idx)\n",
    "    idxs = sorted(set(idxs))\n",
    "    X_shap = X.loc[idxs]\n",
    "    y_shap = y.loc[idxs]\n",
    "\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    records = []\n",
    "    if isinstance(shap_values, list):\n",
    "        classes = list(estimator.classes_)\n",
    "        for i, cls_name in enumerate(classes):\n",
    "            vals = shap_values[i]\n",
    "            mean_abs = np.abs(vals).mean(axis=0)\n",
    "            top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "            for rank, j in enumerate(top_idx, start=1):\n",
    "                records.append({\"CancerType\": str(cls_name), \"Rank\": rank, \"Feature\": X_shap.columns[j], \"Mean|SHAP|\": float(mean_abs[j])})\n",
    "    else:\n",
    "        mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "        top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "        for rank, j in enumerate(top_idx, start=1):\n",
    "            records.append({\"CancerType\": \"ALL\", \"Rank\": rank, \"Feature\": X_shap.columns[j], \"Mean|SHAP|\": float(mean_abs[j])})\n",
    "    pd.DataFrame(records).to_csv(OUT_DIR/\"task2a_top10_features_per_cancer.csv\", index=False)\n",
    "\n",
    "    # (b) Force plots for patient across 5 cancer types\n",
    "    shap.initjs()\n",
    "    # Try to find the patient row; fallback to first row\n",
    "    if id_series is not None and id_series.notna().any():\n",
    "        try:\n",
    "            idx = id_series[id_series.astype(str) == patient_id].index[0]\n",
    "        except Exception:\n",
    "            idx = X.index[0]\n",
    "    else:\n",
    "        idx = X.index[0]\n",
    "    x_row = X.loc[[idx]]\n",
    "    shap_row = explainer.shap_values(x_row)\n",
    "\n",
    "    def save_force_html(path, force_obj):\n",
    "        try:\n",
    "            shap.save_html(str(path), force_obj)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    if isinstance(shap_row, list):\n",
    "        classes = list(estimator.classes_)\n",
    "        for i, cls in enumerate(classes):\n",
    "            expected = explainer.expected_value[i] if isinstance(explainer.expected_value, (list, np.ndarray)) else explainer.expected_value\n",
    "            force = shap.force_plot(expected, shap_row[i][0, :], x_row, feature_names=x_row.columns, matplotlib=False)\n",
    "            save_force_html(OUT_DIR/f\"task2b_forceplot_{cls}_patient_{patient_id.replace(':','-')}.html\", force)\n",
    "    else:\n",
    "        expected = explainer.expected_value\n",
    "        force = shap.force_plot(expected, shap_row[0, :], x_row, feature_names=x_row.columns, matplotlib=False)\n",
    "        save_force_html(OUT_DIR/f\"task2b_forceplot_patient_{patient_id.replace(':','-')}.html\", force)\n",
    "\n",
    "\n",
    "# -------------------- REGRESSION (Tasks 3–4) --------------------\n",
    "\n",
    "def memory_savvy_read_gdsc2(csv_path: str, max_features: int) -> Tuple[pd.DataFrame, pd.Series, pd.Series, Dict[str, int]]:\n",
    "    \"\"\"Load GDSC2 data assuming columns: ['cell_line','drug_name','LN_IC50', <gene features...>].\n",
    "       Returns X (float32), y (float32), keys (cell_line|drug), and some metadata.\n",
    "    \"\"\"\n",
    "    # Read once to find columns (no huge memory)\n",
    "    header_cols = pd.read_csv(csv_path, nrows=0).columns.tolist()\n",
    "    # Heuristics for key/target columns:\n",
    "    target_col = \"LN_IC50\"\n",
    "    id_cols = []\n",
    "    for cand in [\"cell_line\",\"CELL_LINE\",\"CellLine\",\"cellLine\",\"cell_line_name\"]:\n",
    "        if cand in header_cols:\n",
    "            id_cols.append(cand)\n",
    "            break\n",
    "    for cand in [\"drug_name\",\"Drug\",\"DRUG\",\"drug\"]:\n",
    "        if cand in header_cols:\n",
    "            id_cols.append(cand)\n",
    "            break\n",
    "    if target_col not in header_cols:\n",
    "        raise RuntimeError(\"Expected LN_IC50 column missing in GDSC2 CSV.\")\n",
    "    feature_cols = [c for c in header_cols if c not in id_cols+[target_col]]\n",
    "    selected_feature_cols = feature_cols[:max_features]\n",
    "    usecols = id_cols + [target_col] + selected_feature_cols\n",
    "\n",
    "    df = pd.read_csv(csv_path, usecols=usecols)\n",
    "    y = pd.to_numeric(df[target_col], errors=\"coerce\").astype(np.float32)\n",
    "    keys = df[id_cols[0]].astype(str) + \"|\" + df[id_cols[1]].astype(str) if len(id_cols)>=2 else df[id_cols[0]].astype(str)\n",
    "\n",
    "    X = df.drop(columns=[target_col]+id_cols)\n",
    "    for c in X.columns:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.astype(np.float32)\n",
    "    X = X.loc[:, X.notna().any(axis=0)]\n",
    "    meta = {\"n_rows\": int(len(df)), \"n_features\": int(X.shape[1]), \"id_cols\": id_cols, \"target\": target_col}\n",
    "    return X, y, keys, meta\n",
    "\n",
    "\n",
    "def train_compare_regressors(X: pd.DataFrame, y: pd.Series, random_state=RANDOM_STATE) -> Tuple[pd.DataFrame, Dict[str, Pipeline]]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    preprocess = ColumnTransformer([(\"num\", SimpleImputer(strategy=\"median\"), X.columns)], remainder=\"drop\")\n",
    "\n",
    "    models = {}\n",
    "    models[\"DecisionTreeReg\"] = Pipeline([(\"prep\", preprocess), (\"reg\", DecisionTreeRegressor(random_state=random_state, min_samples_leaf=2))])\n",
    "    models[\"RandomForestReg\"] = Pipeline([(\"prep\", preprocess), (\"reg\", RandomForestRegressor(n_estimators=300, random_state=random_state, n_jobs=-1))])\n",
    "    models[\"GBMReg\"] = Pipeline([(\"prep\", preprocess), (\"reg\", GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=random_state))])\n",
    "\n",
    "    if HAVE_XGB:\n",
    "        models[\"XGBReg\"] = Pipeline([(\"prep\", preprocess), (\"reg\", XGBRegressor(n_estimators=600, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, tree_method=\"hist\", n_jobs=-1, random_state=random_state))])\n",
    "    if HAVE_LGBM:\n",
    "        models[\"LGBMReg\"] = Pipeline([(\"prep\", preprocess), (\"reg\", LGBMRegressor(n_estimators=800, learning_rate=0.05, num_leaves=63, subsample=0.8, colsample_bytree=0.8, random_state=random_state))])\n",
    "    if HAVE_CAT:\n",
    "        models[\"CatBoostReg\"] = Pipeline([(\"prep\", preprocess), (\"reg\", CatBoostRegressor(iterations=700, learning_rate=0.05, depth=6, loss_function=\"RMSE\", random_seed=random_state, verbose=False))])\n",
    "\n",
    "    rows = []\n",
    "    fitted = {}\n",
    "    for name, pipe in models.items():\n",
    "        pipe.fit(X_train, y_train)\n",
    "        pred = pipe.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "        rmse = math.sqrt(mse)\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        rows.append({\"Model\": name, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2})\n",
    "        fitted[name] = pipe\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values(by=[\"RMSE\",\"MAE\"], ascending=[True, True]).reset_index(drop=True)\n",
    "    res.to_csv(OUT_DIR/\"task3_regressor_comparison.csv\", index=False)\n",
    "\n",
    "    best_name = res.iloc[0][\"Model\"]\n",
    "    with open(OUT_DIR/\"task3_best_model.txt\",\"w\") as f:\n",
    "        f.write(str(best_name))\n",
    "\n",
    "    return res, fitted\n",
    "\n",
    "\n",
    "def shap_task4(best_reg_model: Pipeline, X: pd.DataFrame, y: pd.Series, keys: pd.Series):\n",
    "    if not HAVE_SHAP:\n",
    "        print(\"SHAP not available — install `shap` to run Task 4.\")\n",
    "        return\n",
    "\n",
    "    est = best_reg_model.named_steps[list(best_reg_model.named_steps.keys())[-1]]\n",
    "    explainer = shap.TreeExplainer(est)\n",
    "\n",
    "    # (a) per-drug top-10 features (requires a 'drug' component in key: cell|drug)\n",
    "    # Parse drug names if keys are \"cell|drug\"\n",
    "    if keys.str.contains(r\"\\|\").any():\n",
    "        drug_names = keys.str.split(\"|\").str[1]\n",
    "    else:\n",
    "        # If single ID, we can't group by drug\n",
    "        drug_names = pd.Series([\"ALL\"]*len(keys), index=keys.index)\n",
    "\n",
    "    # Subsample for speed\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    sample_idx = rng.choice(X.index, size=min(SHAP_SAMPLES_REG, len(X)), replace=False)\n",
    "    X_shap = X.loc[sample_idx]\n",
    "    drugs_shap = drug_names.loc[sample_idx]\n",
    "\n",
    "    shap_vals = explainer.shap_values(X_shap)\n",
    "    # shap_vals shape: (n_samples, n_features) for regression\n",
    "    assert isinstance(shap_vals, np.ndarray) and shap_vals.ndim == 2\n",
    "\n",
    "    # Compute per-drug mean|SHAP|\n",
    "    recs = []\n",
    "    for drug in sorted(drugs_shap.unique()):\n",
    "        mask = (drugs_shap == drug).values\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        mean_abs = np.abs(shap_vals[mask]).mean(axis=0)\n",
    "        top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "        for rank, j in enumerate(top_idx, start=1):\n",
    "            recs.append({\"Drug\": drug, \"Rank\": rank, \"Feature\": X_shap.columns[j], \"Mean|SHAP|\": float(mean_abs[j])})\n",
    "    pd.DataFrame(recs).to_csv(OUT_DIR/\"task4a_top10_features_per_drug.csv\", index=False)\n",
    "\n",
    "    # (b) Least-error pair: compute prediction errors and take min absolute error\n",
    "    preds = best_reg_model.predict(X)\n",
    "    errors = np.abs(preds - y.values)\n",
    "    idx_min = int(np.argmin(errors))\n",
    "    least_key = keys.iloc[idx_min]\n",
    "    # SHAP for that row\n",
    "    x_row = X.iloc[[idx_min]]\n",
    "    row_shap = explainer.shap_values(x_row)[0, :]\n",
    "    mean_abs = np.abs(row_shap)\n",
    "    top_idx = np.argsort(mean_abs)[::-1][:10]\n",
    "    pd.DataFrame({\n",
    "        \"Rank\": np.arange(1, 11),\n",
    "        \"Feature\": X.columns[top_idx],\n",
    "        \"Absolute_SHAP\": mean_abs[top_idx].astype(float)\n",
    "    }).to_csv(OUT_DIR/f\"task4b_top10_features_least_error_{least_key.replace('|','_')}.csv\", index=False)\n",
    "\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== HW3 Runner ===\")\n",
    "\n",
    "    # ---- Tasks 1–2 (Classification) ----\n",
    "    if os.path.exists(CANCER_CSV):\n",
    "        print(\"[Task 1] Loading cancer CSV (memory-optimized)...\")\n",
    "        Xc, yc, ids, target_col, id_col = memory_savvy_read_cancers(CANCER_CSV, MAX_FEATURES_CLASSIF)\n",
    "        print(f\"[Task 1] Loaded: rows={len(Xc)}, features={Xc.shape[1]}, target={target_col}, id={id_col}\")\n",
    "        res_cls, fitted_cls = train_compare_classifiers(Xc, yc, RANDOM_STATE)\n",
    "        best_cls_name = res_cls.iloc[0][\"Model\"]\n",
    "        best_cls = fitted_cls[best_cls_name]\n",
    "        print(f\"[Task 1] Best classifier: {best_cls_name}\")\n",
    "        if HAVE_SHAP:\n",
    "            print(\"[Task 2] Running SHAP for best classifier...\")\n",
    "            shap_task2(best_cls, Xc, yc, ids, PATIENT_ID_TO_PLOT)\n",
    "        else:\n",
    "            print(\"[Task 2] shap not installed — skipping.\")\n",
    "    else:\n",
    "        print(f\"[Task 1-2] Missing {CANCER_CSV}. Put it next to this script.\")\n",
    "\n",
    "    # ---- Tasks 3–4 (Regression) ----\n",
    "    if os.path.exists(GDSC2_CSV):\n",
    "        print(\"[Task 3] Loading GDSC2 CSV (memory-optimized)...\")\n",
    "        Xr, yr, keys, meta = memory_savvy_read_gdsc2(GDSC2_CSV, MAX_FEATURES_REGRESS)\n",
    "        print(f\"[Task 3] Loaded: rows={meta['n_rows']}, features={meta['n_features']} (target={meta['target']}, ids={meta['id_cols']})\")\n",
    "        res_reg, fitted_reg = train_compare_regressors(Xr, yr, RANDOM_STATE)\n",
    "        best_reg_name = res_reg.iloc[0][\"Model\"]\n",
    "        best_reg = fitted_reg[best_reg_name]\n",
    "        print(f\"[Task 3] Best regressor: {best_reg_name}\")\n",
    "        if HAVE_SHAP:\n",
    "            print(\"[Task 4] Running SHAP for best regressor...\")\n",
    "            shap_task4(best_reg, Xr, yr, keys)\n",
    "        else:\n",
    "            print(\"[Task 4] shap not installed — skipping.\")\n",
    "    else:\n",
    "        print(f\"[Task 3-4] Missing {GDSC2_CSV}. Provide it locally to run.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
